from fastapi import Body
from fastapi.responses import StreamingResponse
from app.configs import LLM_MODELS, TEMPERATURE
from app.utils import wrap_done, get_ChatOpenAI, get_prompt_template
from langchain.chains import LLMChain
from langchain.callbacks import AsyncIteratorCallbackHandler
from typing import AsyncIterable
import asyncio
import json
from langchain.prompts.chat import ChatPromptTemplate
from typing import List, Optional, Union
from app.schemas import History
from langchain.prompts import PromptTemplate

from app.memory.conversation_db_buffer_memory import ConversationBufferDBMemory
from app.db.repository.message_repository import add_message_to_db
from app.callback_handler.conversation_callback_handler import (
    ConversationCallbackHandler,
)


async def chat_stream(
    query: str = Body(..., description="User input", examples=["angry"]),
    conversation_id: str = Body("", description="Dialog ID"),
    history_len: int = Body(
        -1, description="Get the number of historical messages from the database"
    ),
    history: Union[int, List[History]] = Body(
        [],
        description="Historical conversation, set to an integer to read historical messages from the database",
        examples=[
            [
                {
                    "role": "user",
                    "content": "write a hello world function",
                },
                {"role": "assistant", "content": "you are a python assistant"},
            ]
        ],
    ),
    stream: bool = Body(False, description="Streaming output"),
    model_name: str = Body(LLM_MODELS[0], description="LLM Model name"),
    temperature: float = Body(
        TEMPERATURE, description="LLM Temperature", ge=0.0, le=1.0
    ),
    max_tokens: Optional[int] = Body(
        None,
        description="Limit the number of tokens generated by LLM. The default value is None, which represents the maximum value of the model.",
    ),
    prompt_name: str = Body(
        "default",
        description="Prompt template name to use(Configure in configs/prompt_config.py",
    ),
):
    async def chat_iterator() -> AsyncIterable[str]:
        nonlocal history, max_tokens
        callback = AsyncIteratorCallbackHandler()
        callbacks = [callback]
        memory = None

        if conversation_id:
            message_id = add_message_to_db(
                chat_type="llm_chat", query=query, conversation_id=conversation_id
            )
            print(f"message_id {message_id}")
            # Responsible for saving llm response to message db
            conversation_callback = ConversationCallbackHandler(
                conversation_id=conversation_id,
                message_id=message_id,
                chat_type="llm_chat",
                query=query,
            )
            callbacks.append(conversation_callback)

        if isinstance(max_tokens, int) and max_tokens <= 0:
            max_tokens = None

        model = get_ChatOpenAI(
            model_name=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
            callbacks=callbacks,
        )

        if (
            history
        ):  # Prioritize the use of historical messages incoming from the front end
            history = [History.from_data(h) for h in history]
            prompt_template = get_prompt_template("llm_chat", prompt_name)
            input_msg = History(role="user", content=prompt_template).to_msg_template(
                False
            )
            chat_prompt = ChatPromptTemplate.from_messages(
                [i.to_msg_template() for i in history] + [input_msg]
            )
        elif (
            conversation_id and history_len > 0
        ):  # The front end requires fetching historical messages from the database
            #
            # When using memory, prompt must contain the variable corresponding to memory.memory_key
            prompt = get_prompt_template("llm_chat", "with_history")
            chat_prompt = PromptTemplate.from_template(prompt)
            # Get the message list based on conversation_id and piece together the memory
            memory = ConversationBufferDBMemory(
                conversation_id=conversation_id, llm=model, message_limit=history_len
            )
        else:
            prompt_template = get_prompt_template("llm_chat", prompt_name)
            input_msg = History(role="user", content=prompt_template).to_msg_template(
                False
            )
            chat_prompt = ChatPromptTemplate.from_messages([input_msg])
        
        #message_id =1
        chain = LLMChain(prompt=chat_prompt, llm=model, memory=memory)

        # Begin a task that runs in the background.
        task = asyncio.create_task(
            wrap_done(chain.acall({"input": query}), callback.done),
        )

        if stream:
            async for token in callback.aiter():
                # Use server-sent-events to stream the response
                yield json.dumps(
                    {"text": token, "message_id": message_id}, ensure_ascii=False
                )
        else:
            answer = ""
            async for token in callback.aiter():
                answer += token
            yield json.dumps(
                {"text": answer, "message_id": message_id}, ensure_ascii=False
            )

        await task

    return StreamingResponse(chat_iterator(), media_type="text/event-stream")
